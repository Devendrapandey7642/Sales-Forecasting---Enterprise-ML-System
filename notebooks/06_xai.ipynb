{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9644af",
   "metadata": {},
   "source": [
    "# 06 - Explainability & Interpretability (XAI)\n",
    "\n",
    "Understand model predictions using SHAP and other interpretability techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f013b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4b2bf",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83513751",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEOFError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load model and scaler\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../models/best_model.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     best_model = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../models/scaler.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      9\u001b[39m     scaler = pickle.load(f)\n",
      "\u001b[31mEOFError\u001b[39m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/processed/featured_dataset.csv')\n",
    "\n",
    "# Load model and scaler with error handling\n",
    "try:\n",
    "    with open('../models/best_model.pkl', 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    print(f\"✓ Model loaded: {type(best_model).__name__}\")\n",
    "except (EOFError, FileNotFoundError, pickle.UnpicklingError) as e:\n",
    "    print(f\"⚠️ Warning: Could not load model - {e}\")\n",
    "    print(\"Train a model first using 04_model_training.ipynb\")\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    best_model = RandomForestRegressor(random_state=42)\n",
    "    print(\"Using placeholder RandomForest model for demonstration\")\n",
    "\n",
    "try:\n",
    "    with open('../models/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "except (EOFError, FileNotFoundError, pickle.UnpicklingError):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Using new StandardScaler for demonstration\")\n",
    "\n",
    "target_col = 'quantity'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Will learn new scaling parameters\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✓ Data prepared: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df58ffe",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357092f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importances,\n",
    "        'importance_normalized': importances / importances.sum()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 Important Features:\")\n",
    "    print(feature_importance_df.head(15).to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    feature_importance_df.head(15).plot(x='feature', y='importance', kind='barh', ax=ax, legend=False)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/xai_feature_importance.png', dpi=100)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model does not have built-in feature importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64921f",
   "metadata": {},
   "source": [
    "## SHAP Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"Computing SHAP values... (this may take a moment)\")\n",
    "\n",
    "try:\n",
    "    if hasattr(best_model, 'predict'):\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "        \n",
    "        print(\"✓ SHAP values computed\")\n",
    "        print(f\"  SHAP shape: {shap_values.shape}\")\n",
    "except:\n",
    "    print(\"Could not compute SHAP for this model type. Trying KernelExplainer...\")\n",
    "    explainer = shap.KernelExplainer(best_model.predict, X_train_scaled[:100])\n",
    "    shap_values = explainer.shap_values(X_test_scaled[:100])\n",
    "    print(\"✓ SHAP values computed (using KernelExplainer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19c8d8",
   "metadata": {},
   "source": [
    "## SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9da228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, show=False, plot_type='bar')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/shap_summary_bar.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bd79d",
   "metadata": {},
   "source": [
    "## SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features for detailed analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(top_features), figsize=(15, 4))\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        plt.sca(axes[idx])\n",
    "        shap.dependence_plot(feature, shap_values, X_test_scaled, feature_names=X.columns, show=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/shap_dependence_plots.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5651b",
   "metadata": {},
   "source": [
    "## Individual Prediction Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a973619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "sample_idx = 0\n",
    "sample_shap = shap_values[sample_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.force_plot(explainer.expected_value, sample_shap, X_test_scaled[sample_idx], feature_names=X.columns, matplotlib=True, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/shap_force_plot.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explanation for sample {sample_idx}:\")\n",
    "print(f\"  Actual value: {y_test.iloc[sample_idx]:.2f}\")\n",
    "print(f\"  Predicted value: {best_model.predict(X_test_scaled[sample_idx:sample_idx+1])[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfc7c9",
   "metadata": {},
   "source": [
    "## Model Behavior Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS FROM MODEL EXPLAINABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i, row in feature_importance_df.head(5).iterrows():\n",
    "        print(f\"  {i+1}. {row['feature']}: {row['importance_normalized']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nModel Behavior:\")\n",
    "print(f\"  - Average prediction: {best_model.predict(X_test_scaled).mean():.2f}\")\n",
    "print(f\"  - Average actual: {y_test.mean():.2f}\")\n",
    "print(f\"  - Prediction std: {best_model.predict(X_test_scaled).std():.2f}\")\n",
    "print(f\"  - Actual std: {y_test.std():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e234d3",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "BUSINESS INSIGHTS & RECOMMENDATIONS:\n",
    "\n",
    "1. **Top Drivers**: Focus on understanding and monitoring the top 5 important features\n",
    "2. **Feature Engineering**: Consider creating interactions between top features\n",
    "3. **Data Collection**: Ensure high-quality data for important features\n",
    "4. **Monitoring**: Set up alerts for anomalies in key predictors\n",
    "5. **Retraining**: Regularly retrain the model as data patterns evolve\n",
    "\n",
    "NEXT STEPS:\n",
    "- Deploy the model to production\n",
    "- Set up monitoring dashboards\n",
    "- Implement feedback loops for continuous improvement\n",
    "- Document findings for stakeholders\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
