{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9644af",
   "metadata": {},
   "source": [
    "# 06 - Explainability & Interpretability (XAI)\n",
    "\n",
    "Understand model predictions using SHAP and other interpretability techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f013b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4b2bf",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83513751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../data/processed/featured_dataset.csv')\n",
    "\n",
    "# Load model and scaler\n",
    "with open('../models/best_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "with open('../models/scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(f\"* Model loaded: {type(best_model).__name__}\")\n",
    "\n",
    "target_col = 'quantity'\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"* Data prepared: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df58ffe",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357092f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importances,\n",
    "        'importance_normalized': importances / importances.sum()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 15 Important Features:\")\n",
    "    print(feature_importance_df.head(15).to_string())\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    feature_importance_df.head(15).plot(x='feature', y='importance', kind='barh', ax=ax, legend=False)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/xai_feature_importance.png', dpi=100)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model does not have built-in feature importances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64921f",
   "metadata": {},
   "source": [
    "## SHAP Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"Computing SHAP values... (this may take a moment)\")\n",
    "\n",
    "try:\n",
    "    if hasattr(best_model, 'predict'):\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_test_scaled)\n",
    "        \n",
    "        print(\"✓ SHAP values computed\")\n",
    "        print(f\"  SHAP shape: {shap_values.shape}\")\n",
    "except:\n",
    "    print(\"Could not compute SHAP for this model type. Trying KernelExplainer...\")\n",
    "    explainer = shap.KernelExplainer(best_model.predict, X_train_scaled[:100])\n",
    "    shap_values = explainer.shap_values(X_test_scaled[:100])\n",
    "    print(\"✓ SHAP values computed (using KernelExplainer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19c8d8",
   "metadata": {},
   "source": [
    "## SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9da228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, show=False, plot_type='bar')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/shap_summary_bar.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bd79d",
   "metadata": {},
   "source": [
    "## SHAP Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c2f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features for detailed analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_features = feature_importance_df.head(3)['feature'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(top_features), figsize=(15, 4))\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        plt.sca(axes[idx])\n",
    "        shap.dependence_plot(feature, shap_values, X_test_scaled, feature_names=X.columns, show=False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/shap_dependence_plots.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5651b",
   "metadata": {},
   "source": [
    "## Individual Prediction Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a973619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a single prediction\n",
    "sample_idx = 0\n",
    "sample_shap = shap_values[sample_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.force_plot(explainer.expected_value, sample_shap, X_test_scaled[sample_idx], feature_names=X.columns, matplotlib=True, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/shap_force_plot.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explanation for sample {sample_idx}:\")\n",
    "print(f\"  Actual value: {y_test.iloc[sample_idx]:.2f}\")\n",
    "print(f\"  Predicted value: {best_model.predict(X_test_scaled[sample_idx:sample_idx+1])[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfc7c9",
   "metadata": {},
   "source": [
    "## Model Behavior Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS FROM MODEL EXPLAINABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i, row in feature_importance_df.head(5).iterrows():\n",
    "        print(f\"  {i+1}. {row['feature']}: {row['importance_normalized']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nModel Behavior:\")\n",
    "print(f\"  - Average prediction: {best_model.predict(X_test_scaled).mean():.2f}\")\n",
    "print(f\"  - Average actual: {y_test.mean():.2f}\")\n",
    "print(f\"  - Prediction std: {best_model.predict(X_test_scaled).std():.2f}\")\n",
    "print(f\"  - Actual std: {y_test.std():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e234d3",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "BUSINESS INSIGHTS & RECOMMENDATIONS:\n",
    "\n",
    "1. **Top Drivers**: Focus on understanding and monitoring the top 5 important features\n",
    "2. **Feature Engineering**: Consider creating interactions between top features\n",
    "3. **Data Collection**: Ensure high-quality data for important features\n",
    "4. **Monitoring**: Set up alerts for anomalies in key predictors\n",
    "5. **Retraining**: Regularly retrain the model as data patterns evolve\n",
    "\n",
    "NEXT STEPS:\n",
    "- Deploy the model to production\n",
    "- Set up monitoring dashboards\n",
    "- Implement feedback loops for continuous improvement\n",
    "- Document findings for stakeholders\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
