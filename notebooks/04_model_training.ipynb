{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9077692e",
   "metadata": {},
   "source": [
    "# 04 - Model Training\n",
    "\n",
    "Train multiple ML/DL models for sales forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src.train import train_model, save_model\n",
    "from src.evaluate import calculate_metrics, print_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df3bde",
   "metadata": {},
   "source": [
    "## Load Featured Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ea9b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (146608, 44)\n",
      "Columns: ['date', 'item_id', 'quantity', 'price_base', 'sum_total', 'store_id', 'division', 'format', 'city', 'area', 'dept_name', 'class_name', 'subclass_name', 'item_type', 'weight_volume', 'weight_netto', 'fatness', 'price', 'code', 'promo_price_before', 'promo_price_after', 'promo_days', 'online_qty', 'online_price', 'online_revenue', 'markdown_normal_price', 'markdown_price', 'markdown_qty', 'year', 'month', 'quarter', 'day_of_week', 'day_of_month', 'week_of_year', 'quantity_lag_7', 'sum_total_lag_7', 'quantity_lag_30', 'sum_total_lag_30', 'quantity_rolling_mean_7', 'sum_total_rolling_mean_7', 'quantity_rolling_mean_14', 'sum_total_rolling_mean_14', 'quantity_rolling_mean_30', 'sum_total_rolling_mean_30']\n",
      "\n",
      "First rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price_base</th>\n",
       "      <th>sum_total</th>\n",
       "      <th>store_id</th>\n",
       "      <th>division</th>\n",
       "      <th>format</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>...</th>\n",
       "      <th>quantity_lag_7</th>\n",
       "      <th>sum_total_lag_7</th>\n",
       "      <th>quantity_lag_30</th>\n",
       "      <th>sum_total_lag_30</th>\n",
       "      <th>quantity_rolling_mean_7</th>\n",
       "      <th>sum_total_rolling_mean_7</th>\n",
       "      <th>quantity_rolling_mean_14</th>\n",
       "      <th>sum_total_rolling_mean_14</th>\n",
       "      <th>quantity_rolling_mean_30</th>\n",
       "      <th>sum_total_rolling_mean_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-23</td>\n",
       "      <td>0be3804714de</td>\n",
       "      <td>8.000</td>\n",
       "      <td>129.90</td>\n",
       "      <td>1039.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Div1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>City1</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>15.000</td>\n",
       "      <td>1948.50</td>\n",
       "      <td>4.000</td>\n",
       "      <td>389.70</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>927.728571</td>\n",
       "      <td>10.785714</td>\n",
       "      <td>1390.882143</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>1209.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-15</td>\n",
       "      <td>0be3804714de</td>\n",
       "      <td>3.000</td>\n",
       "      <td>129.90</td>\n",
       "      <td>389.70</td>\n",
       "      <td>1</td>\n",
       "      <td>Div1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>City1</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>15.000</td>\n",
       "      <td>1947.60</td>\n",
       "      <td>10.000</td>\n",
       "      <td>1280.61</td>\n",
       "      <td>5.428571</td>\n",
       "      <td>705.171429</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>1196.771429</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1179.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>18d0dd039cd8</td>\n",
       "      <td>3.332</td>\n",
       "      <td>699.90</td>\n",
       "      <td>2332.08</td>\n",
       "      <td>1</td>\n",
       "      <td>Div1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>City1</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.516</td>\n",
       "      <td>1061.05</td>\n",
       "      <td>0.374</td>\n",
       "      <td>246.80</td>\n",
       "      <td>6.827429</td>\n",
       "      <td>3097.511429</td>\n",
       "      <td>5.230714</td>\n",
       "      <td>2523.572857</td>\n",
       "      <td>5.209267</td>\n",
       "      <td>2605.126333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-30</td>\n",
       "      <td>b55a901fb8ff</td>\n",
       "      <td>7.000</td>\n",
       "      <td>59.89</td>\n",
       "      <td>419.20</td>\n",
       "      <td>1</td>\n",
       "      <td>Div1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>City1</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000</td>\n",
       "      <td>406.68</td>\n",
       "      <td>2.000</td>\n",
       "      <td>119.80</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>223.922857</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>276.397143</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>239.049667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-22</td>\n",
       "      <td>0be3804714de</td>\n",
       "      <td>25.000</td>\n",
       "      <td>129.90</td>\n",
       "      <td>3247.50</td>\n",
       "      <td>1</td>\n",
       "      <td>Div1</td>\n",
       "      <td>Regular</td>\n",
       "      <td>City1</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000</td>\n",
       "      <td>779.40</td>\n",
       "      <td>3.000</td>\n",
       "      <td>389.70</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>1057.757143</td>\n",
       "      <td>9.428571</td>\n",
       "      <td>1224.642857</td>\n",
       "      <td>9.933333</td>\n",
       "      <td>1274.916000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       item_id  quantity  price_base  sum_total  store_id  \\\n",
       "0  2023-08-23  0be3804714de     8.000      129.90    1039.20         1   \n",
       "1  2023-03-15  0be3804714de     3.000      129.90     389.70         1   \n",
       "2  2022-12-30  18d0dd039cd8     3.332      699.90    2332.08         1   \n",
       "3  2023-08-30  b55a901fb8ff     7.000       59.89     419.20         1   \n",
       "4  2024-08-22  0be3804714de    25.000      129.90    3247.50         1   \n",
       "\n",
       "  division   format   city  area  ... quantity_lag_7 sum_total_lag_7  \\\n",
       "0     Div1  Regular  City1  1200  ...         15.000         1948.50   \n",
       "1     Div1  Regular  City1  1200  ...         15.000         1947.60   \n",
       "2     Div1  Regular  City1  1200  ...          1.516         1061.05   \n",
       "3     Div1  Regular  City1  1200  ...          6.000          406.68   \n",
       "4     Div1  Regular  City1  1200  ...          6.000          779.40   \n",
       "\n",
       "  quantity_lag_30 sum_total_lag_30  quantity_rolling_mean_7  \\\n",
       "0           4.000           389.70                 7.142857   \n",
       "1          10.000          1280.61                 5.428571   \n",
       "2           0.374           246.80                 6.827429   \n",
       "3           2.000           119.80                 3.571429   \n",
       "4           3.000           389.70                 8.142857   \n",
       "\n",
       "   sum_total_rolling_mean_7  quantity_rolling_mean_14  \\\n",
       "0                927.728571                 10.785714   \n",
       "1                705.171429                  9.285714   \n",
       "2               3097.511429                  5.230714   \n",
       "3                223.922857                  4.428571   \n",
       "4               1057.757143                  9.428571   \n",
       "\n",
       "   sum_total_rolling_mean_14  quantity_rolling_mean_30  \\\n",
       "0                1390.882143                  9.433333   \n",
       "1                1196.771429                  9.200000   \n",
       "2                2523.572857                  5.209267   \n",
       "3                 276.397143                  3.900000   \n",
       "4                1224.642857                  9.933333   \n",
       "\n",
       "   sum_total_rolling_mean_30  \n",
       "0                1209.353000  \n",
       "1                1179.656000  \n",
       "2                2605.126333  \n",
       "3                 239.049667  \n",
       "4                1274.916000  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/featured_dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bba6d",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e726c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (146608, 43)\n",
      "Target shape: (146608,)\n",
      "Target statistics:\n",
      "count    146608.00000\n",
      "mean          9.53005\n",
      "std          42.07140\n",
      "min         -28.00000\n",
      "25%           2.00000\n",
      "50%           3.25500\n",
      "75%           8.00000\n",
      "max        4243.00000\n",
      "Name: quantity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define target and features\n",
    "target_col = 'quantity'  # Target variable for sales forecasting\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target statistics:\")\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a6790",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24537e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 117286 samples\n",
      "Test size: 29322 samples\n",
      "Training ratio: 80.0%\n",
      "Test ratio: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test size: {X_test.shape[0]} samples\")\n",
    "print(f\"Training ratio: {X_train.shape[0] / len(df) * 100:.1f}%\")\n",
    "print(f\"Test ratio: {X_test.shape[0] / len(df) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9942a83",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb95a1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2024-01-28'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_9768\\1225769819.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train_scaled = scaler.fit_transform(X_train)\n\u001b[32m      3\u001b[39m X_test_scaled = scaler.transform(X_test)\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m print(f\"âœ“ Features scaled\")\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m                 )\n\u001b[32m    904\u001b[39m \n\u001b[32m    905\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    908\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    910\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    920\u001b[39m             Fitted scaler.\n\u001b[32m    921\u001b[39m         \"\"\"\n\u001b[32m    922\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    923\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    957\u001b[39m             Fitted scaler.\n\u001b[32m    958\u001b[39m         \"\"\"\n\u001b[32m    959\u001b[39m         xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m    960\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m         X = validate_data(\n\u001b[32m    962\u001b[39m             self,\n\u001b[32m    963\u001b[39m             X,\n\u001b[32m    964\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2898\u001b[39m             out = y\n\u001b[32m   2899\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m             out = X, y\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2903\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1019\u001b[39m                         )\n\u001b[32m   1020\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1022\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m                 raise ValueError(\n\u001b[32m   1025\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    875\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    876\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    879\u001b[39m \n\u001b[32m    880\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2024-01-28'"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"âœ“ Features scaled\")\n",
    "print(f\"Train mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Train std: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Save scaler\n",
    "with open('../models/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"âœ“ Scaler saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95a782",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef916200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "\n",
      "1. Linear Regression...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1. Linear Regression...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m lr_model = LinearRegression()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m lr_model.fit(\u001b[43mX_train_scaled\u001b[49m, y_train)\n\u001b[32m     10\u001b[39m models[\u001b[33m'\u001b[39m\u001b[33mlinear_regression\u001b[39m\u001b[33m'\u001b[39m] = lr_model\n\u001b[32m     11\u001b[39m y_pred_lr = lr_model.predict(X_test_scaled)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Train multiple models\n",
    "models = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"1. Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "models['linear_regression'] = lr_model\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "print_metrics(calculate_metrics(y_test, y_pred_lr))\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2. Random Forest Regressor...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "models['random_forest'] = rf_model\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "print_metrics(calculate_metrics(y_test, y_pred_rf))\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "print(\"\\n3. Gradient Boosting Regressor...\")\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "models['gradient_boosting'] = gb_model\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "print_metrics(calculate_metrics(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8cbbe",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6671ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'R2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModel Comparison:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(results_df)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m best_model_name = \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mR2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.idxmax()\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ† Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m best_model = models[best_model_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dp686\\Desktop\\sales-forecasting\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'R2'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    results[name] = {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "best_model_name = results_df['R2'].idxmax()\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "best_model = models[best_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c921ff3",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1651e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m model_path = \u001b[33m'\u001b[39m\u001b[33m../models/best_model.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     pickle.dump(\u001b[43mbest_model\u001b[49m, f)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ“ Best model saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Model type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(best_model).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "model_path = '../models/best_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"âœ“ Best model saved: {model_path}\")\n",
    "print(f\"  Model type: {type(best_model).__name__}\")\n",
    "print(f\"  RÂ² Score: {results_df.loc[best_model_name, 'R2']:.4f}\")\n",
    "print(f\"  RMSE: {results_df.loc[best_model_name, 'RMSE']:.4f}\")\n",
    "print(f\"  MAE: {results_df.loc[best_model_name, 'MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae840de",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e91293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mbest_model\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mfeature_importances_\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m     importances = best_model.feature_importances_\n\u001b[32m      3\u001b[39m     feature_importance_df = pd.DataFrame({\n\u001b[32m      4\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m'\u001b[39m: X.columns,\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m: importances\n\u001b[32m      6\u001b[39m     }).sort_values(\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "    \n",
    "    # Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    feature_importance_df.head(10).plot(x='feature', y='importance', kind='barh', ax=ax)\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/feature_importance.png', dpi=100)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Selected model does not have feature importances\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
