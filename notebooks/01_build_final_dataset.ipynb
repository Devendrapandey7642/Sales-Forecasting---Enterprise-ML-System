{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec2010b",
   "metadata": {},
   "source": [
    "# 01 - Build Final Dataset\n",
    "\n",
    "Merge all raw CSV files into one master dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b805a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from data_pipeline import load_raw_data, merge_datasets, handle_missing_values, build_final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6d1b1",
   "metadata": {},
   "source": [
    "## Load All Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156d1d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load all raw files using the pipeline function\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dfs = \u001b[43mload_raw_data\u001b[49m(\u001b[33m'\u001b[39m\u001b[33m../data/raw\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Total files loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dfs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Load all raw files using the pipeline function\n",
    "try:\n",
    "    dfs = load_raw_data('../data/raw')\n",
    "    print(f\"[OK] Total files loaded: {len(dfs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load raw data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b92c62",
   "metadata": {},
   "source": [
    "## Sales LONG format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a63ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã ACTUAL COLUMN NAMES IN EACH DATASET:\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check actual column names in each dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìã ACTUAL COLUMN NAMES IN EACH DATASET:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdfs\u001b[49m.items():\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "# Check actual column names in each dataset\n",
    "print(\"üìã ACTUAL COLUMN NAMES IN EACH DATASET:\\n\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name.upper()}:\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970fca3b",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b97a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SAMPLE DATA:\n",
      "\n",
      "SALES (main transaction data):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä SAMPLE DATA:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSALES (main transaction data):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdfs\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msales\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m3\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSTORES (store info):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(dfs[\u001b[33m'\u001b[39m\u001b[33mstores\u001b[39m\u001b[33m'\u001b[39m].head())\n",
      "\u001b[31mNameError\u001b[39m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's look at sample data from key files\n",
    "print(\"\\nüìä SAMPLE DATA:\\n\")\n",
    "print(\"SALES (main transaction data):\")\n",
    "print(dfs['sales'].head(3))\n",
    "print(\"\\nSTORES (store info):\")\n",
    "print(dfs['stores'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f7c32",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f81e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úì DATA FORMAT CHECK\n",
      "======================================================================\n",
      "Data is already in LONG FORMAT ‚úì\n",
      "  No need for pivot/melt operations\n",
      "  Each row = 1 transaction on 1 date\n",
      "  We can directly merge on (item_id, store_id, date)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úì Data is ALREADY in long format (not wide format)\n",
    "# Each row = one transaction (date, item_id, store_id, quantity, price)\n",
    "# So we DON'T need to melt/pivot\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì DATA FORMAT CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Data is already in LONG FORMAT ‚úì\")\n",
    "print(f\"  No need for pivot/melt operations\")\n",
    "print(f\"  Each row = 1 transaction on 1 date\")\n",
    "print(f\"  We can directly merge on (item_id, store_id, date)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243a784",
   "metadata": {},
   "source": [
    "## Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40046da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ BUILDING MASTER DATASET FROM ALL 8 CSV FILES\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'merge_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ BUILDING MASTER DATASET FROM ALL 8 CSV FILES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m final_df = \u001b[43mmerge_datasets\u001b[49m(dfs)\n\u001b[32m      7\u001b[39m final_df = handle_missing_values(final_df)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Save to processed folder\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'merge_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the complete pipeline to build final dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ BUILDING MASTER DATASET FROM ALL 8 CSV FILES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "final_df = merge_datasets(dfs)\n",
    "final_df = handle_missing_values(final_df)\n",
    "\n",
    "# Save to processed folder\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "final_df.to_csv('../data/processed/final_dataset.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Master dataset built and saved!\")\n",
    "print(f\"üìä Final dataset shape: {final_df.shape}\")\n",
    "print(f\"üìÅ Saved to: ../data/processed/final_dataset.csv\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
